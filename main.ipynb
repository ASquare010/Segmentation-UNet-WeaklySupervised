{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb50d5e9",
   "metadata": {},
   "source": [
    "# Note Dowload the dataset\n",
    "---\n",
    "Save it as data in root folder https://www.kaggle.com/datasets/balraj98/deepglobe-land-cover-classification-dataset\n",
    "folder structure should be ->\n",
    "\n",
    "```\n",
    "ðŸ“¦data\n",
    " â”£ ðŸ“‚test\n",
    " â”ƒ â”£ ðŸ“œ100877_sat.jpg\n",
    " â”ƒ â”£ ðŸ“œ103215_sat.jpg\n",
    " â”£ ðŸ“‚train\n",
    " â”ƒ â”£ ðŸ“œ100694_mask.png\n",
    " â”ƒ â”£ ðŸ“œ100694_sat.jpg\n",
    " â”£ ðŸ“‚valid\n",
    " â”ƒ â”£ ðŸ“œ105036_sat.jpg\n",
    " â”£ ðŸ“œarchive.zip\n",
    " â”£ ðŸ“œclass_dict.csv\n",
    " â”— ðŸ“œmetadata.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9117e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision numpy pandas pillow matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf50bc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39487db7",
   "metadata": {},
   "source": [
    "### Notebook Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c3cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color mapping (from class_dict.csv)\n",
    "COLOR_MAP = {\n",
    "    (0, 255, 255): 0,  # urban_land\n",
    "    (255, 255, 0): 1,  # agriculture_land\n",
    "    (255, 0, 255): 2,  # rangeland\n",
    "    (0, 255, 0): 3,  # forest_land\n",
    "    (0, 0, 255): 4,  # water\n",
    "    (255, 255, 255): 5,  # barren_land\n",
    "    (0, 0, 0): 6,  # unknown\n",
    "}\n",
    "# SETTINGS - change these\n",
    "LR = 1e-3\n",
    "EPOCHS = 1\n",
    "ROOT = \"data\"  # path where metadata.csv and train/valid/test folders live\n",
    "METADATA = \"metadata.csv\"\n",
    "NUM_CLASSES = 7\n",
    "IGNORE_CLASS = 6  # class id for 'unknown' from your class_dict (mark as ignore)\n",
    "IGNORE_INDEX = 255  # sentinel for unlabeled pixels (PartialCE)\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 0\n",
    "IMAGE_SIZE = (512, 512)\n",
    "POINTS_RATIO_PER_CLASS = 0.1  # how many points/class to sample during training\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SAVE_DIR = \"checkpoints\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "PRE_COMPUTED_WEIGHTS_CLASSES = torch.tensor(\n",
    "    [0.0353, 0.0066, 0.0456, 0.0327, 0.1170, 0.0441, 0.0000], dtype=torch.float32,device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c09c7",
   "metadata": {},
   "source": [
    "### Dataset class â€” simple, works from lists of paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8240f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepGlobeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that returns (image_tensor, mask_tensor) where:\n",
    "      - image_tensor: (3, H, W) float tensor (0..1 or normalized if transform included)\n",
    "      - mask_tensor: (H, W) long tensor with class indices [0..6]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_paths: List[str],\n",
    "        mask_paths: List[str],\n",
    "        transform: Optional[callable] = None,\n",
    "        target_transform: Optional[callable] = None,\n",
    "    ):\n",
    "        assert len(image_paths) == len(mask_paths)\n",
    "        self.image_paths = list(image_paths)\n",
    "        self.mask_paths = list(mask_paths)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def _rgb_to_class(self, mask_img: Image.Image) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert a PIL RGB mask to a single-channel long tensor of class indices.\n",
    "        \"\"\"\n",
    "        mask_np = np.array(mask_img)  # (H, W, 3)\n",
    "        label_mask = np.zeros(mask_np.shape[:2], dtype=np.int64)\n",
    "        for rgb, idx in COLOR_MAP.items():\n",
    "            rgb_arr = np.array(rgb, dtype=np.uint8).reshape(1, 1, 3)\n",
    "            matches = np.all(mask_np == rgb_arr, axis=-1)\n",
    "            label_mask[matches] = int(idx)\n",
    "        return torch.from_numpy(label_mask).long()  # (H, W)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_p = self.image_paths[idx]\n",
    "        mk_p = self.mask_paths[idx]\n",
    "\n",
    "        img = Image.open(img_p).convert(\"RGB\")\n",
    "        mk = Image.open(mk_p).convert(\"RGB\")\n",
    "\n",
    "        # image transform (e.g., Resize, ToTensor, Normalize)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = T.ToTensor()(img)\n",
    "\n",
    "        # convert color-coded mask -> class indices\n",
    "        mask_idx = self._rgb_to_class(mk)\n",
    "\n",
    "        if self.target_transform:\n",
    "            mask_idx = self.target_transform(mask_idx)\n",
    "\n",
    "        return img, mask_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8868e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_transform_fn(mask_tensor_or_pil, image_size=(512, 512)):\n",
    "    \"\"\"\n",
    "    Resize mask (nearest neighbor) and return (H, W) long tensor.\n",
    "    Accepts either Pillow.Image or torch.Tensor (single-channel or int mask).\n",
    "    \"\"\"\n",
    "    if isinstance(mask_tensor_or_pil, torch.Tensor):\n",
    "        arr = mask_tensor_or_pil.cpu().numpy().astype(np.uint8)\n",
    "        pil = Image.fromarray(arr)\n",
    "    else:\n",
    "        pil = mask_tensor_or_pil\n",
    "    resized = pil.resize(image_size, resample=Image.NEAREST)\n",
    "    return torch.from_numpy(np.array(resized)).long()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Helper to build train/val datasets from metadata\n",
    "# ---------------------------\n",
    "def build_train_val_datasets(\n",
    "    root: str,\n",
    "    metadata_file: str = \"metadata.csv\",\n",
    "    val_ratio: float = 0.1,\n",
    "    seed: int = 42,\n",
    "    image_size: Tuple[int, int] = (512, 512),\n",
    "    val_split_name: str = \"valid\",\n",
    "    require_masks_exist: bool = True,\n",
    ") -> Tuple[DeepGlobeDataset, DeepGlobeDataset]:\n",
    "    \"\"\"\n",
    "    Read metadata.csv and build (train_ds, val_ds).\n",
    "    - If `valid` rows with masks exist, use them.\n",
    "    - Otherwise split 'train' rows into train/val with val_ratio.\n",
    "    This function creates sensible default transforms (image + mask) using image_size.\n",
    "    \"\"\"\n",
    "    meta_path = os.path.join(root, metadata_file)\n",
    "    df = pd.read_csv(meta_path)\n",
    "\n",
    "    # minimal cleaning\n",
    "    df = df.dropna(subset=[\"sat_image_path\", \"mask_path\"])\n",
    "    df[\"sat_image_path\"] = df[\"sat_image_path\"].astype(str)\n",
    "    df[\"mask_path\"] = df[\"mask_path\"].astype(str)\n",
    "\n",
    "    # default transforms created here (inside function)\n",
    "    img_tf = T.Compose(\n",
    "        [\n",
    "            T.Resize(image_size),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    # bind image_size into mask_transform via partial (top-level function used)\n",
    "    mask_tf = partial(mask_transform_fn, image_size=image_size)\n",
    "\n",
    "    def _pairs_from_df(subdf):\n",
    "        imgs = [os.path.join(root, p) for p in subdf[\"sat_image_path\"].tolist()]\n",
    "        masks = [os.path.join(root, p) for p in subdf[\"mask_path\"].tolist()]\n",
    "        pairs = [\n",
    "            (i, m)\n",
    "            for i, m in zip(imgs, masks)\n",
    "            if (not require_masks_exist) or (os.path.exists(i) and os.path.exists(m))\n",
    "        ]\n",
    "        return pairs\n",
    "\n",
    "    # attempt to use provided valid split if available\n",
    "    valid_df = df[df[\"split\"] == val_split_name]\n",
    "    valid_pairs = _pairs_from_df(valid_df)\n",
    "    if len(valid_pairs) > 0:\n",
    "        train_pairs = _pairs_from_df(df[df[\"split\"] == \"train\"])\n",
    "        print(\n",
    "            f\"[INFO] Using provided valid split: {len(train_pairs)} train / {len(valid_pairs)} valid\"\n",
    "        )\n",
    "    else:\n",
    "        # fallback: split train into train+val\n",
    "        train_df = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "        pairs = _pairs_from_df(train_df)\n",
    "        if len(pairs) == 0:\n",
    "            raise ValueError(\n",
    "                \"No train pairs found on disk. Check metadata / file locations.\"\n",
    "            )\n",
    "        rng = np.random.RandomState(seed)\n",
    "        perm = rng.permutation(len(pairs))\n",
    "        n_val = max(1, int(len(pairs) * val_ratio))\n",
    "        val_idx = perm[:n_val]\n",
    "        train_idx = perm[n_val:]\n",
    "        train_pairs = [pairs[i] for i in train_idx]\n",
    "        valid_pairs = [pairs[i] for i in val_idx]\n",
    "        print(\n",
    "            f\"[INFO] No valid split provided -> split train into {len(train_pairs)} train / {len(valid_pairs)} val (val_ratio={val_ratio})\"\n",
    "        )\n",
    "\n",
    "    # unzip\n",
    "    train_imgs, train_masks = zip(*train_pairs)\n",
    "    val_imgs, val_masks = zip(*valid_pairs)\n",
    "\n",
    "    # build datasets with the transforms created inside the function\n",
    "    train_ds = DeepGlobeDataset(\n",
    "        list(train_imgs), list(train_masks), transform=img_tf, target_transform=mask_tf\n",
    "    )\n",
    "    val_ds = DeepGlobeDataset(\n",
    "        list(val_imgs), list(val_masks), transform=img_tf, target_transform=mask_tf\n",
    "    )\n",
    "\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_partial_labels(image, full_mask, partial_mask, class_colors=None):\n",
    "    \"\"\"\n",
    "    Shows the original image, full mask, and partial (sampled) mask.\n",
    "    \"\"\"\n",
    "    # Detach tensors\n",
    "    image = image.detach().cpu()\n",
    "    full_mask = full_mask.detach().cpu().numpy()\n",
    "    partial_mask = partial_mask.detach().cpu().numpy()\n",
    "\n",
    "    # Convert image to HWC\n",
    "    img_vis = image.permute(1, 2, 0).numpy()\n",
    "    img_vis = np.clip(img_vis, 0, 1)\n",
    "\n",
    "    # Define class colors if not provided\n",
    "    if class_colors is None:\n",
    "        class_colors = {\n",
    "            0: [0, 255, 255],\n",
    "            1: [255, 255, 0],\n",
    "            2: [255, 0, 255],\n",
    "            3: [0, 255, 0],\n",
    "            4: [0, 0, 255],\n",
    "            5: [255, 255, 255],\n",
    "            6: [0, 0, 0],\n",
    "            255: [0, 0, 0],  # ignore (unlabeled)\n",
    "        }\n",
    "\n",
    "    def mask_to_rgb(mask):\n",
    "        rgb = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
    "        for cls, color in class_colors.items():\n",
    "            rgb[mask == cls] = color\n",
    "        return rgb\n",
    "\n",
    "    mask_rgb = mask_to_rgb(full_mask)\n",
    "    partial_rgb = mask_to_rgb(partial_mask)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img_vis)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(mask_rgb)\n",
    "    plt.title(\"Full Ground Truth Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(partial_rgb)\n",
    "    plt.title(\"Sampled Partial Labels\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70107db1",
   "metadata": {},
   "source": [
    "### Model Lightweight UNet (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1042000",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_ch=3, num_classes=7, base_filters=32):\n",
    "        super().__init__()\n",
    "        f = base_filters\n",
    "        self.enc1 = ConvBlock(in_ch, f)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = ConvBlock(f, f * 2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = ConvBlock(f * 2, f * 4)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.enc4 = ConvBlock(f * 4, f * 8)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = ConvBlock(f * 8, f * 16)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(f * 16, f * 8, 2, stride=2)\n",
    "        self.dec4 = ConvBlock(f * 16, f * 8)\n",
    "        self.up3 = nn.ConvTranspose2d(f * 8, f * 4, 2, stride=2)\n",
    "        self.dec3 = ConvBlock(f * 8, f * 4)\n",
    "        self.up2 = nn.ConvTranspose2d(f * 4, f * 2, 2, stride=2)\n",
    "        self.dec2 = ConvBlock(f * 4, f * 2)\n",
    "        self.up1 = nn.ConvTranspose2d(f * 2, f, 2, stride=2)\n",
    "        self.dec1 = ConvBlock(f * 2, f)\n",
    "\n",
    "        self.head = nn.Conv2d(f, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "        e4 = self.enc4(self.pool3(e3))\n",
    "\n",
    "        b = self.bottleneck(self.pool4(e4))\n",
    "\n",
    "        d4 = self.up4(b)\n",
    "        d4 = torch.cat([d4, e4], dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "\n",
    "        d3 = self.up3(d4)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "\n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        out = self.head(d1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb1c60",
   "metadata": {},
   "source": [
    "### Partial Cross Entropy Loss (masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1682427",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialCrossEntropyLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Partial Cross Entropy Loss for segmentation with sparse labels.\n",
    "    Supports optional class weighting computed from dataset pixel frequencies.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): Dataset used to compute class weights (if weight=None).\n",
    "        ignore_index (int): Label to ignore in loss computation.\n",
    "        weight (torch.Tensor | list | np.ndarray | None): Precomputed class weights (shape: [num_classes]).\n",
    "        reduction (str): One of {'none', 'mean', 'sum'}.\n",
    "        label_smoothing (float): Optional label smoothing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset=None,\n",
    "        ignore_index: int = 255,\n",
    "        weight: torch.Tensor = None,\n",
    "        reduction: str = \"mean\",\n",
    "        label_smoothing: float = 0.0,\n",
    "        num_classes: int = 7,\n",
    "        ignore_label: int = 6,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert reduction in (\"none\", \"mean\", \"sum\")\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduction = reduction\n",
    "        self.label_smoothing = float(label_smoothing)\n",
    "\n",
    "        if weight is not None:\n",
    "            # rename to avoid shadowing builtins\n",
    "            w = (\n",
    "                weight.clone()\n",
    "                if isinstance(weight, torch.Tensor)\n",
    "                else torch.tensor(weight, dtype=torch.float32)\n",
    "            )\n",
    "            self.register_buffer(\"_weight\", w)\n",
    "            print(\n",
    "                f\"[INFO] Using precomputed class weights: {torch.round(w, decimals=3).tolist()}\"\n",
    "            )\n",
    "\n",
    "        elif dataset is not None:\n",
    "            print(\n",
    "                \"[INFO] No weights provided â€” computing class weights from dataset ...\"\n",
    "            )\n",
    "            w = self.compute_class_weights_from_dataset(\n",
    "                dataset=dataset,\n",
    "                num_classes=num_classes,\n",
    "                ignore_label=ignore_label,\n",
    "                normalize=True,\n",
    "                mode=\"inverse_freq\",\n",
    "            )\n",
    "        else:\n",
    "            self.register_buffer(\"_weight\", None)\n",
    "            print(\n",
    "                \"[WARN] No class weights or dataset provided â€” using uniform weighting.\"\n",
    "            )\n",
    "\n",
    "    # --------------------------\n",
    "    @property\n",
    "    def weight(self):\n",
    "        \"\"\"Access the registered weight tensor.\"\"\"\n",
    "        return self._weight\n",
    "\n",
    "    # -----------------------------------\n",
    "    # Compute class weights from dataset\n",
    "    # -----------------------------------\n",
    "    @torch.no_grad()\n",
    "    def compute_class_weights_from_dataset(\n",
    "        self,\n",
    "        dataset,\n",
    "        num_classes: int,\n",
    "        ignore_label: int = None,\n",
    "        normalize: bool = True,\n",
    "        mode: str = \"inverse_freq\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute and set class weights automatically from dataset pixel counts.\n",
    "\n",
    "        Args:\n",
    "            dataset: A dataset yielding (image, mask) where mask is (H, W) tensor.\n",
    "            num_classes (int): Total number of semantic classes.\n",
    "            ignore_label (int, optional): Class to ignore (weight = 0).\n",
    "            normalize (bool): Whether to normalize weights so mean = 1.\n",
    "            mode (str): 'inverse_freq' or 'log_inv_freq'\n",
    "        \"\"\"\n",
    "        counts = np.zeros(num_classes, dtype=np.float64)\n",
    "\n",
    "        for i in range(len(dataset)):\n",
    "            _, mask = dataset[i]\n",
    "            mask = mask.cpu().numpy().ravel()\n",
    "            for c in range(num_classes):\n",
    "                counts[c] += (mask == c).sum()\n",
    "\n",
    "        # avoid divide by zero\n",
    "        counts = np.maximum(counts, 1e-6)\n",
    "\n",
    "        if mode == \"inverse_freq\":\n",
    "            weights = 1.0 / counts\n",
    "        elif mode == \"log_inv_freq\":\n",
    "            weights = 1.0 / np.log(1.02 + counts)\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'inverse_freq' or 'log_inv_freq'\")\n",
    "\n",
    "        if normalize:\n",
    "            weights = weights / weights.mean()\n",
    "\n",
    "        if ignore_label is not None and ignore_label < len(weights):\n",
    "            weights[ignore_label] = 0.0\n",
    "\n",
    "        print(\"[INFO] Computed class weights:\", np.round(weights, 3).tolist())\n",
    "        return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "    # --------------------------\n",
    "    # Forward (standard partial CE)\n",
    "    # --------------------------\n",
    "    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        if inputs.dim() != 4:\n",
    "            raise ValueError(\"inputs must be (B, C, H, W)\")\n",
    "        if targets.dim() != 3:\n",
    "            raise ValueError(\"targets must be (B, H, W)\")\n",
    "\n",
    "        B, C, H, W = inputs.shape\n",
    "        logits = inputs.permute(0, 2, 3, 1).reshape(-1, C)\n",
    "        flat_targets = targets.reshape(-1)\n",
    "\n",
    "        mask = flat_targets != self.ignore_index\n",
    "        if mask.sum() == 0:\n",
    "            zero = torch.tensor(0.0, dtype=inputs.dtype, device=inputs.device)\n",
    "            if self.reduction == \"none\":\n",
    "                return zero.new_zeros((0,))\n",
    "            return zero\n",
    "\n",
    "        logits_masked = logits[mask]\n",
    "        targets_masked = flat_targets[mask]\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            logits_masked,\n",
    "            targets_masked,\n",
    "            weight=self.weight,\n",
    "            reduction=\"none\",\n",
    "            label_smoothing=self.label_smoothing,\n",
    "        )\n",
    "\n",
    "        if self.reduction == \"none\":\n",
    "            return loss\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42d9d79",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944067cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Point-sampler (simulate partial labels)\n",
    "# -------------------------\n",
    "def sample_point_labels(\n",
    "    full_mask: torch.Tensor,\n",
    "    keep_ratio: float = 0.01,  # e.g. keep 1% of labeled pixels\n",
    "    ignore_index: int = 255,\n",
    "    classes: list = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Randomly sample a subset of labeled pixels from a full segmentation mask,\n",
    "    keeping a given percentage of pixels per class.\n",
    "\n",
    "    This simulates *partial supervision* by retaining only a fraction of pixels\n",
    "    from each class, while marking the rest as unlabeled (ignore_index).\n",
    "\n",
    "    Args:\n",
    "        full_mask (torch.Tensor):\n",
    "            Ground-truth segmentation mask.\n",
    "            Expected shapes: (H, W), (1, H, W), or (H, W, 1).\n",
    "            Each pixel contains a class ID.\n",
    "        keep_ratio (float):\n",
    "            Fraction of pixels to retain per class (e.g., 0.01 = 1% of pixels for each class).\n",
    "            Sampling is done independently per class to preserve class balance.\n",
    "        ignore_index (int):\n",
    "            Label value used to mark unlabeled pixels.\n",
    "            These are ignored during loss computation (e.g., in PartialCrossEntropyLoss).\n",
    "        classes (list, optional):\n",
    "            Specific class IDs to sample from. If None, inferred from the mask.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor:\n",
    "            A tensor of shape (H, W) where:\n",
    "              - Sampled pixels retain their original class IDs\n",
    "              - All other pixels are set to `ignore_index`\n",
    "\n",
    "    Notes:\n",
    "        - Sampling is performed *per class* (each class keeps the same ratio of its own pixels).\n",
    "        - Ensures at least one pixel is kept per class, even for rare classes.\n",
    "        - Useful for simulating point-level or sparse segmentation supervision.\n",
    "\n",
    "    Example:\n",
    "        >>> mask = torch.randint(0, 5, (512, 512))\n",
    "        >>> sampled = sample_point_labels(mask, keep_ratio=0.02)\n",
    "        >>> (sampled != 255).float().mean()\n",
    "        tensor(0.0201)  # ~2% of pixels retained per class\n",
    "    \"\"\"\n",
    "    # --- Normalize shape ---\n",
    "    if full_mask.ndim == 4:  # (B, 1, H, W)\n",
    "        full_mask = full_mask.squeeze(0)\n",
    "    if full_mask.ndim == 3:\n",
    "        if full_mask.shape[0] == 1:\n",
    "            full_mask = full_mask.squeeze(0)\n",
    "        elif full_mask.shape[-1] == 1:\n",
    "            full_mask = full_mask.squeeze(-1)\n",
    "        else:\n",
    "            # If RGB mask (e.g., color-coded), take first channel (should already be converted before)\n",
    "            full_mask = full_mask[0]\n",
    "    elif full_mask.ndim > 4 or full_mask.ndim < 2:\n",
    "        raise ValueError(f\"Unexpected mask shape: {full_mask.shape}\")\n",
    "\n",
    "    H, W = full_mask.shape[-2], full_mask.shape[-1]\n",
    "    full_mask = full_mask.long()\n",
    "\n",
    "    # --- Initialize mask with ignore_index ---\n",
    "    point_mask = torch.full(\n",
    "        (H, W), ignore_index, dtype=torch.long, device=full_mask.device\n",
    "    )\n",
    "\n",
    "    # --- Determine classes present ---\n",
    "    unique = torch.unique(full_mask)\n",
    "    if classes is not None:\n",
    "        unique = [c for c in unique if c in classes]\n",
    "\n",
    "    # --- Sample by ratio ---\n",
    "    for c in unique:\n",
    "        if c == ignore_index:\n",
    "            continue\n",
    "        ys, xs = torch.where(full_mask == c)\n",
    "        if len(ys) == 0:\n",
    "            continue\n",
    "        num_keep = max(1, int(len(ys) * keep_ratio))  # at least 1 pixel\n",
    "        idx = torch.randperm(len(ys))[:num_keep]\n",
    "        point_mask[ys[idx], xs[idx]] = c\n",
    "\n",
    "    return point_mask\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# mIoU metric for validation\n",
    "# -------------------------\n",
    "def compute_confusion_matrix(\n",
    "    preds: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    num_classes: int,\n",
    "    ignore_index: int = 255,\n",
    "):\n",
    "    \"\"\"\n",
    "    preds: (N, H, W) long\n",
    "    targets: (N, H, W) long\n",
    "    Returns: confusion matrix shape (num_classes, num_classes)\n",
    "    \"\"\"\n",
    "    mask = targets != ignore_index\n",
    "    preds = preds[mask]\n",
    "    targets = targets[mask]\n",
    "    if targets.numel() == 0:\n",
    "        return torch.zeros((num_classes, num_classes), dtype=torch.long)\n",
    "    k = (targets >= 0) & (targets < num_classes)\n",
    "    preds = preds[k]\n",
    "    targets = targets[k]\n",
    "    conf = torch.zeros((num_classes, num_classes), dtype=torch.long)\n",
    "    idx = num_classes * targets + preds\n",
    "    vals, counts = torch.unique(idx, return_counts=True)\n",
    "    for v, c in zip(vals.tolist(), counts.tolist()):\n",
    "        t = v // num_classes\n",
    "        p = v % num_classes\n",
    "        conf[t, p] += c\n",
    "    return conf\n",
    "\n",
    "\n",
    "def compute_miou_from_conf(conf_matrix: torch.Tensor, ignore_classes: List[int] = None):\n",
    "    if ignore_classes is None:\n",
    "        ignore_classes = []\n",
    "    num_classes = conf_matrix.shape[0]\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        if cls in ignore_classes:\n",
    "            ious.append(float(\"nan\"))\n",
    "            continue\n",
    "        tp = conf_matrix[cls, cls].item()\n",
    "        fn = conf_matrix[cls, :].sum().item() - tp\n",
    "        fp = conf_matrix[:, cls].sum().item() - tp\n",
    "        denom = tp + fp + fn\n",
    "        iou = tp / denom if denom > 0 else float(\"nan\")\n",
    "        ious.append(iou)\n",
    "    # mean over valid classes\n",
    "    valid = [\n",
    "        x\n",
    "        for i, x in enumerate(ious)\n",
    "        if i not in (ignore_classes or []) and not np.isnan(x)\n",
    "    ]\n",
    "    miou = float(np.nanmean(valid)) if len(valid) > 0 else 0.0\n",
    "    return miou, ious\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Training / Validation loops\n",
    "# -------------------------\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, keep_ratio_per_class):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "    for images, masks in loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # Create point supervision masks\n",
    "        batch_point_masks = []\n",
    "        for b in range(images.size(0)):\n",
    "            pm = sample_point_labels(\n",
    "                masks[b],\n",
    "                keep_ratio=keep_ratio_per_class,\n",
    "                ignore_index=criterion.ignore_index,\n",
    "            )\n",
    "            batch_point_masks.append(pm)\n",
    "        point_masks = torch.stack(batch_point_masks, dim=0)  # (B, H, W)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)  # (B, C, H, W)\n",
    "        loss = criterion(logits, point_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    return running_loss / max(1, n_batches)\n",
    "\n",
    "\n",
    "def validate(\n",
    "    model, loader, device, num_classes, ignore_index=255, ignore_eval_classes=None\n",
    "):\n",
    "    model.eval()\n",
    "    conf = torch.zeros((num_classes, num_classes), dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        for images, masks in loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            logits = model(images)\n",
    "            preds = logits.argmax(dim=1)  # (B, H, W)\n",
    "            conf_batch = compute_confusion_matrix(\n",
    "                preds.cpu(), masks.cpu(), num_classes, ignore_index=ignore_index\n",
    "            )\n",
    "            conf += conf_batch\n",
    "    miou, ious = compute_miou_from_conf(conf, ignore_classes=ignore_eval_classes)\n",
    "    return miou, ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    train_ds, val_ds = build_train_val_datasets(\n",
    "        root=ROOT,\n",
    "        metadata_file=METADATA,\n",
    "        val_ratio=0.1,\n",
    "        seed=42,\n",
    "        image_size=IMAGE_SIZE,\n",
    "    )\n",
    "\n",
    "    print(\"Train samples:\", len(train_ds))\n",
    "    print(\"Val samples:\", len(val_ds))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Model, loss, optimizer\n",
    "    criterion = PartialCrossEntropyLoss(\n",
    "        weight=PRE_COMPUTED_WEIGHTS_CLASSES,\n",
    "        ignore_index=255,\n",
    "        reduction=\"mean\",\n",
    "    )\n",
    "    model = UNet(in_ch=3, num_classes=NUM_CLASSES, base_filters=32).to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    best_miou = -1.0\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        t0 = time.time()\n",
    "        train_loss = train_one_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            DEVICE,\n",
    "            keep_ratio_per_class=POINTS_RATIO_PER_CLASS,\n",
    "        )\n",
    "        val_miou, val_ious = validate(\n",
    "            model,\n",
    "            val_loader,\n",
    "            DEVICE,\n",
    "            NUM_CLASSES,\n",
    "            ignore_index=IGNORE_INDEX,\n",
    "            ignore_eval_classes=[IGNORE_CLASS],\n",
    "        )\n",
    "        scheduler.step()\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        print(\n",
    "            f\"Epoch {epoch}/{EPOCHS} â€” train_loss: {train_loss:.4f} â€” val_mIoU: {val_miou:.4f} â€” time: {elapsed:.1f}s\"\n",
    "        )\n",
    "        print(\n",
    "            \" per-class IoU:\",\n",
    "            [\"{:.3f}\".format(x) if not np.isnan(x) else \"nan\" for x in val_ious],\n",
    "        )\n",
    "\n",
    "        # Save best\n",
    "        if val_miou > best_miou:\n",
    "            best_miou = val_miou\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"miou\": val_miou,\n",
    "                },\n",
    "                os.path.join(SAVE_DIR, f\"unet_partial_best.pth\"),\n",
    "            )\n",
    "        break\n",
    "    print(\"Training complete. Best mIoU:\", best_miou)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3170883f",
   "metadata": {},
   "source": [
    "## Inspect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcf6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = build_train_val_datasets(\n",
    "    root=ROOT,\n",
    "    metadata_file=METADATA,\n",
    "    val_ratio=0.1,\n",
    "    seed=42,\n",
    "    image_size=(512, 512),\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)}, Val samples: {len(val_ds)}\")\n",
    "img, mask = train_ds[1]\n",
    "print(\n",
    "    f\"Image shape: {img.shape}, Mask shape: {mask.shape}, Unique labels: {torch.unique(mask)}\"\n",
    ")\n",
    "\n",
    "# Sample sparse partial labels (keep 5% pixels per class)\n",
    "partial_mask = sample_point_labels(\n",
    "    mask, keep_ratio=POINTS_RATIO_PER_CLASS, ignore_index=255\n",
    ")\n",
    "print(\"Partial mask unique labels:\", partial_mask.unique())\n",
    "visualize_partial_labels(img, mask, partial_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2264451",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bea2e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
